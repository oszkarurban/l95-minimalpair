{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tacJMv7p1rD"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch\n",
        "!pip install conllu\n",
        "!pip install minicons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyxuWCGGuOia"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "import conllu\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from minicons import scorer\n",
        "from collections import Counter\n",
        "import math\n",
        "import gzip\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB6FpBJgAaLE"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/UniversalDependencies/UD_Hungarian-Szeged/master/hu_szeged-ud-train.conllu\n",
        "!wget https://raw.githubusercontent.com/UniversalDependencies/UD_Hungarian-Szeged/master/hu_szeged-ud-dev.conllu\n",
        "!wget https://raw.githubusercontent.com/UniversalDependencies/UD_Hungarian-Szeged/master/hu_szeged-ud-test.conllu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZjRL3JbKF4Wk"
      },
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "\n",
        "def get_negation_data_pools(filepaths):\n",
        "    negation_pairs = []\n",
        "\n",
        "    for path in filepaths:\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = conllu.parse(f.read())\n",
        "\n",
        "            for sentence in data:\n",
        "                id_to_token = {t['id']: t for t in sentence}\n",
        "\n",
        "                for token in sentence:\n",
        "                    if token['deprel'] == 'compound:preverb':\n",
        "                        head_id = token['head']\n",
        "                        if head_id in id_to_token:\n",
        "                            head = id_to_token[head_id]\n",
        "\n",
        "                            if head['upos'] == 'VERB':\n",
        "                                mood = head.get('feats', {}).get('Mood', 'Ind')\n",
        "\n",
        "                                feats = head.get('feats', {})\n",
        "                                person = feats.get('Person', '')\n",
        "                                number = feats.get('Number', '')\n",
        "                                tense = feats.get('Tense', '')\n",
        "                                definite = feats.get('Definite', '')\n",
        "\n",
        "                                negation_pairs.append({\n",
        "                                    'preverb': token['form'].lower(),\n",
        "                                    'verb': head['form'].lower(),\n",
        "                                    'mood': mood,\n",
        "                                    'person': person,\n",
        "                                    'number': number,\n",
        "                                    'tense': tense,\n",
        "                                    'definite': definite\n",
        "                                })\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: File {path} not found.\")\n",
        "\n",
        "    unique_pairs = list({(p['preverb'], p['verb'], p['mood']): p for p in negation_pairs}.values())\n",
        "    print(f\"unique pairs found in data: {len(unique_pairs)}\")\n",
        "    return unique_pairs\n",
        "\n",
        "def generate_negation_evaluation_suite(pairs, target_count=500):\n",
        "    random.shuffle(pairs)\n",
        "    selected_pairs = pairs[:target_count]\n",
        "\n",
        "    dataset = []\n",
        "    for pair in selected_pairs:\n",
        "        if pair['mood'] == 'Imp':\n",
        "            op = \"Ne\"\n",
        "            op_type = \"Prohibitive\"\n",
        "        else:\n",
        "            op = \"Nem\"\n",
        "            op_type = \"Declarative\"\n",
        "\n",
        "        gram = f\"{op} {pair['verb']} {pair['preverb']}\"\n",
        "        ungram = f\"{op} {pair['preverb']}{pair['verb']}\"\n",
        "\n",
        "        dataset.append({\n",
        "            'phenomenon': 'Negation',\n",
        "            'preverb': pair['preverb'],\n",
        "            'verb': pair['verb'],\n",
        "            'grammatical': gram,\n",
        "            'ungrammatical': ungram,\n",
        "            'operator_type': op_type,\n",
        "            'mood_detected': pair['mood'],\n",
        "            'person': pair['person'],\n",
        "            'number': pair['number'],\n",
        "            'tense': pair['tense'],\n",
        "            'definite': pair['definite']\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(dataset)\n",
        "\n",
        "    print(\"preerb frquency\")\n",
        "    freq_table = df['preverb'].value_counts().reset_index()\n",
        "    freq_table.columns = ['Preverb', 'Count']\n",
        "    print(freq_table.to_string(index=False))\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaZtnEuNF7N8"
      },
      "outputs": [],
      "source": [
        "files = [\n",
        "    \"hu_szeged-ud-train.conllu\",\n",
        "    \"hu_szeged-ud-dev.conllu\",\n",
        "    \"hu_szeged-ud-test.conllu\"\n",
        "]\n",
        "\n",
        "pairs_pool = get_negation_data_pools(files)\n",
        "df_final = generate_negation_evaluation_suite(pairs_pool, target_count=500)\n",
        "\n",
        "df_final.to_csv(\"hungarian_negation_minimal_pairs.csv\", index=False, encoding='utf-8-sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VIIyo7nVLLT"
      },
      "outputs": [],
      "source": [
        "def build_webcorpus_unigrams(tokenizer, max_tokens=20_000_000):\n",
        "    base_url = \"https://nessie.ilab.sztaki.hu/~ndavid/Webcorpus2_text/\"\n",
        "    token_counts = Counter()\n",
        "    total_tokens = 0\n",
        "\n",
        "    for i in tqdm(range(1, 101), desc=\"Webcorpus 2.0\"):\n",
        "        filename = f\"2017_2018_{i:04d}.txt.gz\"\n",
        "        local_file = f\"webcorpus_{i:04d}.txt.gz\"\n",
        "\n",
        "        if total_tokens > max_tokens:\n",
        "            break\n",
        "\n",
        "        if not os.path.exists(local_file):\n",
        "          url = f\"{base_url}{filename}\"\n",
        "          urllib.request.urlretrieve(url, local_file)\n",
        "\n",
        "\n",
        "        with gzip.open(local_file, 'rt', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    tokens = tokenizer(line)['input_ids']\n",
        "                    token_counts.update(tokens)\n",
        "                    total_tokens += len(tokens)\n",
        "\n",
        "    unigram_logprobs = {tid: math.log(count / total_tokens)\n",
        "                       for tid, count in token_counts.items()}\n",
        "\n",
        "    torch.save(unigram_logprobs, \"model_hun_webcorpus2_unigrams.pt\")\n",
        "    return unigram_logprobs\n",
        "\n",
        "#or with babylm for multilingual model\n",
        "model_id = \"goldfish-models/hun_latn_1000mb\" #\"BabyLM-community/ALL-baseline-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "unigram_path = \"model_hun_webcorpus2_unigrams.pt\"\n",
        "\n",
        "try:\n",
        "    unigram_logprobs = torch.load(unigram_path, map_location=\"cpu\")\n",
        "    print(f\"Loaded {len(unigram_logprobs)} webcorpus 2.0 unigrams\")\n",
        "except FileNotFoundError:\n",
        "    unigram_logprobs = build_webcorpus_unigrams(tokenizer)\n",
        "\n",
        "model_raw = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        "ilm_model = scorer.IncrementalLMScorer(model_raw, \"cuda\" if torch.cuda.is_available() else \"cpu\", tokenizer=tokenizer)\n",
        "\n",
        "def score_pair(ilm_model, sen, wrong_sen):\n",
        "    return ilm_model.sequence_score([sen, wrong_sen], reduction=lambda x: x)\n",
        "\n",
        "def run_eval(model, df, tokenizer, unigrams, column_grammatical, column_ungrammatical):\n",
        "    results = df.copy()\n",
        "    oov_backoff = -5  # for tokens not in unigram sample, not even used based on statistics\n",
        "    total_oov_count = 0\n",
        "    total_tokens_processed = 0\n",
        "\n",
        "    for idx, row in tqdm(results.iterrows(), total=len(results)):\n",
        "        sen_prob, wrong_prob = score_pair(model, row[column_grammatical], row[column_ungrammatical])\n",
        "        sen_tokens = tokenizer(row[column_grammatical])['input_ids']\n",
        "        wrong_tokens = tokenizer(row[column_ungrammatical])['input_ids']\n",
        "        sen_lp_sum = sen_prob.sum().item()\n",
        "        wrong_lp_sum = wrong_prob.sum().item()\n",
        "        sen_len = len(sen_prob)\n",
        "        wrong_len = len(wrong_prob)\n",
        "\n",
        "\n",
        "        sen_uni_vals = [unigrams.get(tid, oov_backoff) for tid in sen_tokens[1:]]\n",
        "        wrong_uni_vals = [unigrams.get(tid, oov_backoff) for tid in wrong_tokens[1:]]\n",
        "        sen_oovs = sen_uni_vals.count(oov_backoff)\n",
        "        wrong_oovs = wrong_uni_vals.count(oov_backoff)\n",
        "\n",
        "        total_oov_count += (sen_oovs + wrong_oovs)\n",
        "        total_tokens_processed += (len(sen_uni_vals) + len(wrong_uni_vals))\n",
        "        sen_uni_sum = sum(unigrams.get(tid, oov_backoff) for tid in sen_tokens[1:])\n",
        "        wrong_uni_sum = sum(unigrams.get(tid, oov_backoff) for tid in wrong_tokens[1:])\n",
        "\n",
        "        #metrics\n",
        "        sen_lp_norm = sen_lp_sum / sen_len\n",
        "        wrong_lp_norm = wrong_lp_sum / wrong_len\n",
        "\n",
        "        sen_slor = (sen_lp_sum - sen_uni_sum) / sen_len\n",
        "        wrong_slor = (wrong_lp_sum - wrong_uni_sum) / wrong_len\n",
        "\n",
        "        results.at[idx, 'sen_lp_norm'] = sen_lp_norm\n",
        "        results.at[idx, 'wrong_lp_norm'] = wrong_lp_norm\n",
        "        results.at[idx, 'sen_slor'] = sen_slor\n",
        "        results.at[idx, 'wrong_slor'] = wrong_slor\n",
        "        results.at[idx, 'sen_nll'] = -sen_lp_sum\n",
        "        results.at[idx, 'wrong_nll'] = -wrong_lp_sum\n",
        "\n",
        "    results['nll_correct'] = (results['wrong_nll'] - results['sen_nll']) > 0\n",
        "    results['norm_correct'] = (results['sen_lp_norm'] - results['wrong_lp_norm']) > 0\n",
        "    results['slor_correct'] = (results['sen_slor'] - results['wrong_slor']) > 0\n",
        "\n",
        "    oov_rate = (total_oov_count / total_tokens_processed) * 100\n",
        "    print(f\"Total Tokens Checked: {total_tokens_processed}\")\n",
        "    print(f\"Total OOV Backoffs:   {total_oov_count}\")\n",
        "    print(f\"OOV Rate:             {oov_rate:.4f}%\")\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgeVe_j3YEyQ"
      },
      "outputs": [],
      "source": [
        "experiments = [\n",
        "    {\n",
        "        \"version\": \"Bare\",\n",
        "        \"file\": \"./hungarian_negation_preverb_minimal_pair.csv\",\n",
        "        \"col_good\": \"grammatical\",\n",
        "        \"col_bad\": \"ungrammatical\"\n",
        "    },\n",
        "    {\n",
        "        \"version\": \"Context\",\n",
        "        \"file\": \"./hungarian_negation_preverb_minimal_pair.csv\",\n",
        "        \"col_good\": \"grammatical_context\",\n",
        "        \"col_bad\": \"ungrammatical_context\"\n",
        "    },\n",
        "    {\n",
        "        \"version\": \"Intervener\",\n",
        "        \"file\": \"./hungarian_negation_preverb_minimal_pair.csv\",\n",
        "        \"col_good\": \"grammatical_context_adverb\",\n",
        "        \"col_bad\": \"ungrammatical_context_adverb\"\n",
        "    },\n",
        "    {\n",
        "        \"version\": \"Intervener2\",\n",
        "        \"file\": \"./hungarian_negation_preverb_minimal_pair.csv\",\n",
        "        \"col_good\": \"grammatical_context_adverb2\",\n",
        "        \"col_bad\": \"ungrammatical_context_adverb2\"\n",
        "    }\n",
        "]\n",
        "all_detailed_results = []\n",
        "for exp in experiments:\n",
        "  v_name = exp['version']\n",
        "  f_path = exp['file']\n",
        "  col_g = exp['col_good']\n",
        "  col_b = exp['col_bad']\n",
        "\n",
        "  df_input = pd.read_csv(f_path, encoding='utf-8')\n",
        "  column_grammatical = col_g\n",
        "  column_ungrammatical = col_b\n",
        "\n",
        "  print(f\"Experiment {f_path}\")\n",
        "  res_df = run_eval(ilm_model, df_input, tokenizer, unigram_logprobs, column_grammatical, column_ungrammatical)\n",
        "  res_df['version'] = v_name\n",
        "  res_df['model_name'] = \"Goldfish\"\n",
        "\n",
        "  cols_to_keep = ['verb',\n",
        "                  'preverb',\n",
        "                  'version',\n",
        "                  'model_name',\n",
        "                   col_g,\n",
        "                   col_b,\n",
        "                   'nll_correct',\n",
        "                   'norm_correct',\n",
        "                   'slor_correct',\n",
        "                   'sen_nll',\n",
        "                   'sen_lp_norm',\n",
        "                   'sen_slor',\n",
        "                   'wrong_nll',\n",
        "                   'wrong_lp_norm',\n",
        "                   'wrong_slor'\n",
        "                   ]\n",
        "\n",
        "  res_df_clean = res_df[cols_to_keep]\n",
        "  all_detailed_results.append(res_df_clean)\n",
        "\n",
        "\n",
        "master_df = pd.concat(all_detailed_results, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fahSLE5FVV5d"
      },
      "outputs": [],
      "source": [
        "def create_metric_summary_table(master_df):\n",
        "    summary = master_df.groupby(['model_name', 'version'])[[\n",
        "        'nll_correct',\n",
        "        'norm_correct',\n",
        "        'slor_correct'\n",
        "    ]].mean() * 100\n",
        "\n",
        "\n",
        "    summary = summary.rename(columns={\n",
        "        'nll_correct': 'NLL Accuracy (%)',\n",
        "        'norm_correct': 'Length-Norm Accuracy (%)',\n",
        "        'slor_correct': 'SLOR Accuracy (%)',\n",
        "    })\n",
        "\n",
        "    summary = summary.round(2)\n",
        "    summary = summary.reset_index()\n",
        "\n",
        "    return summary\n",
        "\n",
        "metric_table = create_metric_summary_table(master_df)\n",
        "display(metric_table)\n",
        "\n",
        "# print(metric_table.to_latex(index=False, float_format=\"%.2f\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD0nEzvZcbPB"
      },
      "outputs": [],
      "source": [
        "def preverb_type(master_df, metric='slor_correct'):\n",
        "    linguistic_map = {\n",
        "        #Pure Aspectual\n",
        "        'meg': 'Aspectual Preverbs',\n",
        "        'el': 'Aspectual Preverbs',\n",
        "        'újra': 'Aspectual Preverbs',\n",
        "\n",
        "        #Spatial / Directional\n",
        "        'ki': 'Spatial/Directional',\n",
        "        'be': 'Spatial/Directional',\n",
        "        'fel': 'Spatial/Directional',\n",
        "        'föl': 'Spatial/Directional',\n",
        "        'le': 'Spatial/Directional',\n",
        "        'át': 'Spatial/Directional',\n",
        "        'elő': 'Spatial/Directional',\n",
        "        'előre': 'Spatial/Directional',\n",
        "        'vissza': 'Spatial/Directional',\n",
        "        'össze': 'Spatial/Directional',\n",
        "        'hozzá': 'Spatial/Directional',\n",
        "        'rá': 'Spatial/Directional',\n",
        "        'bele': 'Spatial/Directional',\n",
        "        'alá': 'Spatial/Directional',\n",
        "        'szét': 'Spatial/Directional',\n",
        "        'haza': 'Spatial/Directional',\n",
        "        'szembe': 'Spatial/Directional',\n",
        "        'végig': 'Spatial/Directional',\n",
        "        'oda': 'Spatial/Directional',\n",
        "        'felül': 'Spatial/Directional',\n",
        "        'hátra': 'Spatial/Directional',\n",
        "        'tovább': 'Spatial/Directional',\n",
        "\n",
        "        # Idiomatic\n",
        "        'végre': 'Figurative Preverbs',\n",
        "        'észre': 'Figurative Preverbs',\n",
        "        'közzé': 'Figurative Preverbs',\n",
        "        'utol': 'Figurative Preverbs',\n",
        "        'létre': 'Figurative Preverbs',\n",
        "        'közre': 'Figurative Preverbs',\n",
        "        'egyet': 'Figurative Preverbs',\n",
        "        'együtt': 'Figurative Preverbs',\n",
        "        'végbe': 'Figurative Preverbs',\n",
        "        'jóvá': 'Figurative Preverbs'\n",
        "    }\n",
        "\n",
        "    plot_df = master_df.copy()\n",
        "    plot_df['Linguistic_Category'] = plot_df['preverb'].map(linguistic_map)\n",
        "    # plot_df = plot_df.dropna(subset=['Linguistic_Category'])\n",
        "    summary = plot_df.groupby(['Linguistic_Category', 'version'])[metric].mean().reset_index()\n",
        "\n",
        "    order = ['Bare', 'Context', 'Intervener', 'Intervener2']\n",
        "    summary['version'] = pd.Categorical(summary['version'], categories=order, ordered=True)\n",
        "    summary['Accuracy (%)'] = summary[metric] * 100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #plot\n",
        "    plt.figure(figsize=(11, 7))\n",
        "    sns.lineplot(\n",
        "        data=summary,\n",
        "        x='version',\n",
        "        y='Accuracy (%)',\n",
        "        hue='Linguistic_Category',\n",
        "        style='Linguistic_Category',\n",
        "        markers=True,\n",
        "        dashes=False,\n",
        "        linewidth=3,\n",
        "        palette={\n",
        "            'Spatial/Directional': '#2980b9',\n",
        "            'Aspectual Preverbs': '#27ae60',\n",
        "            'Figurative Preverbs': '#e74c3c'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    plt.title(f\"Minimal Pair Accuracy by Preverb Type\", fontsize=15) #,fontweight='bold')\n",
        "    plt.ylabel(\"Accuracy (%)\", fontsize=15)\n",
        "    plt.xlabel(\"Experiment type\", fontsize=16\n",
        "               )\n",
        "    plt.ylim(0, 100)\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "    plt.legend(title=\"Preverb Category\", loc='lower left')\n",
        "\n",
        "    plt.xticks(\n",
        "        ticks=[0, 1, 2, 3],\n",
        "        labels=[\"Bare\\n\", \"Context\\n\", \"Intervener 1\\n(1 Adverb)\", \"Intervener 2\\n(2 Adverbs)\"]\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    counts = plot_df['Linguistic_Category'].value_counts()\n",
        "    print(counts.to_markdown())\n",
        "\n",
        "preverb_type(master_df, metric='slor_correct')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nmQJ5QejaD2"
      },
      "outputs": [],
      "source": [
        "####CONFIDENCE####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REJQcrYxfhlF",
        "outputId": "ccc32c88-bd22-4d95-d590-d13afbfe9993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            nll_margin norm_margin slor_margin\n",
            "                  mean        mean        mean\n",
            "version                                       \n",
            "Bare          4.028735    1.494739    1.187768\n",
            "Context       4.111027    0.786007    0.599397\n",
            "Intervener    2.795898    0.535657    0.366652\n",
            "Intervener2  -0.024317    0.173260    0.023617\n"
          ]
        }
      ],
      "source": [
        "# NLL: lower is better\n",
        "master_df['nll_margin'] = master_df['wrong_nll'] - master_df['sen_nll']\n",
        "\n",
        "# Length-Norm\n",
        "master_df['norm_margin'] = master_df['sen_lp_norm'] - master_df['wrong_lp_norm']\n",
        "\n",
        "# SLOR:\n",
        "master_df['slor_margin'] = master_df['sen_slor'] - master_df['wrong_slor']\n",
        "\n",
        "metrics = ['nll_margin', 'norm_margin', 'slor_margin']\n",
        "print(master_df.groupby('version')[metrics].agg(['mean']))#, 'std', 'median']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVWxK6z1jXja"
      },
      "outputs": [],
      "source": [
        "######TOKENIZATION####"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_global(master_df, tokenizer, metric='slor_correct'):\n",
        "    col_map = {\n",
        "        \"Bare\": (\"grammatical\", \"ungrammatical\"),\n",
        "        \"Context\": (\"grammatical_context\", \"ungrammatical_context\"),\n",
        "        \"Intervener\": (\"grammatical_context_adverb\", \"ungrammatical_context_adverb\"),\n",
        "        \"Intervener2\": (\"grammatical_context_adverb2\", \"ungrammatical_context_adverb2\")\n",
        "    }\n",
        "\n",
        "    def determine_category(row):\n",
        "        v_type = row['version']\n",
        "        if v_type not in col_map: return pd.Series([None]*2)\n",
        "\n",
        "        pv_token = tokenizer.tokenize(f\" {row['preverb']}\")[-1]\n",
        "\n",
        "        toks_good = tokenizer.tokenize(str(row[col_map[v_type][0]]))\n",
        "        toks_bad = tokenizer.tokenize(str(row[col_map[v_type][1]]))\n",
        "\n",
        "        good_is_split = any(t == pv_token for t in toks_good)\n",
        "        bad_is_split = any(t == pv_token for t in toks_bad)\n",
        "\n",
        "        if good_is_split and not bad_is_split:\n",
        "            category = \"WELL-SPLIT\"\n",
        "        else:\n",
        "            category = \"REST\"\n",
        "\n",
        "        return pd.Series({'category': category, 'is_correct': row[metric]})\n",
        "\n",
        "    res = master_df.apply(determine_category, axis=1)\n",
        "    df_merged = pd.concat([master_df, res], axis=1).dropna(subset=['category'])\n",
        "\n",
        "    summary = df_merged.groupby('category')['is_correct'].agg(['mean', 'count']).reset_index()\n",
        "    summary.columns = ['Category', 'Accuracy', 'Count']\n",
        "    summary['Accuracy (%)'] = (summary['Accuracy'] * 100).round(2)\n",
        "\n",
        "    return summary.sort_values('Accuracy', ascending=False), df_merged\n",
        "\n",
        "summary_stats, detailed_df = tokenize_global(master_df, tokenizer)\n",
        "print(summary_stats.to_markdown(index=False))"
      ],
      "metadata": {
        "id": "76FWO2sFXJ_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_bare(master_df, tokenizer, metric='slor_correct'):\n",
        "\n",
        "    df_bare = master_df[master_df['version'] == 'Bare'].copy()\n",
        "    col_map = {\n",
        "        \"Bare\": (\"grammatical\", \"ungrammatical\")\n",
        "    }\n",
        "\n",
        "    def determine_category(row):\n",
        "        v_type = row['version']\n",
        "        if v_type not in col_map:\n",
        "            return pd.Series({'category': None, 'toks_good': None, 'toks_bad': None})\n",
        "\n",
        "\n",
        "        pv_token = tokenizer.tokenize(f\" {row['preverb']}\")[-1]\n",
        "\n",
        "        toks_good = tokenizer.tokenize(str(row[col_map[v_type][0]]))\n",
        "        toks_bad = tokenizer.tokenize(str(row[col_map[v_type][1]]))\n",
        "\n",
        "        good_is_split = any(t == pv_token for t in toks_good)\n",
        "        bad_is_split = any(t == pv_token for t in toks_bad)\n",
        "\n",
        "        if good_is_split and not bad_is_split:\n",
        "            category = \"WELL-SPLIT\"\n",
        "        else:\n",
        "            category = \"REST\"\n",
        "\n",
        "        return pd.Series({\n",
        "            'category': category,\n",
        "            'is_correct': row[metric],\n",
        "            'toks_good': toks_good,\n",
        "            'toks_bad': toks_bad\n",
        "        })\n",
        "\n",
        "    results = df_bare.apply(determine_category, axis=1)\n",
        "    df_merged = pd.concat([master_df, results], axis=1).dropna(subset=['category'])\n",
        "\n",
        "    summary = df_merged.groupby('category')['is_correct'].agg(['mean', 'count']).reset_index()\n",
        "    summary.columns = ['Category', 'Accuracy', 'Count']\n",
        "    summary['Accuracy%'] = (summary['Accuracy'] * 100).round(2)\n",
        "    print(\"statistic (bare)\")\n",
        "    print(summary.to_markdown(index=False))\n",
        "\n",
        "    print(f\"\\n ==== WELL-SPLIT\")\n",
        "    successes = df_merged[(df_merged['category'] == \"WELL-SPLIT\") & (df_merged['is_correct'] == True)].head(10)\n",
        "    for _, r in successes.iterrows():\n",
        "        print(f\"\\nPair: {r['preverb']} + {r['verb']} | CORRECT\")\n",
        "        print(f\"  Ungramm: {r['toks_bad']}\")\n",
        "        print(f\"  Gramm:   {r['toks_good']}\")\n",
        "\n",
        "    print(f\"\\n ==== WELL-SPLIT (INCORRECT)\")\n",
        "    failures = df_merged[(df_merged['category'] == \"WELL-SPLIT\") & (df_merged['is_correct'] == False)]\n",
        "    if failures.empty:\n",
        "        print(\"No Well-Split failures found.\")\n",
        "    else:\n",
        "        for _, r in failures.iterrows():\n",
        "            print(f\"\\nPair: {r['preverb']} + {r['verb']} | INCORRECT\")\n",
        "            print(f\"  Ungramm: {r['toks_bad']}\")\n",
        "        print(f\"  Gramm:   {r['toks_good']}\")\n",
        "\n",
        "    print(f\"\\n ==== REST\")\n",
        "    rest_examples = df_merged[df_merged['category'] == \"REST\"]\n",
        "    for _, r in rest_examples.iterrows():\n",
        "        status = \"CORRECT\" if r['is_correct'] else \"INCORRECT\"\n",
        "        print(f\"\\nPair: {r['preverb']} + {r['verb']} | {status}\")\n",
        "        print(f\"  Ungramm: {r['toks_bad']}\")\n",
        "        print(f\"  Gramm:   {r['toks_good']}\")\n",
        "\n",
        "    return summary, df_merged\n",
        "\n",
        "summary, detailed_df = tokenize_bare(master_df, tokenizer)"
      ],
      "metadata": {
        "id": "JdDpYq0FfGS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB7b6U6oCVQq"
      },
      "outputs": [],
      "source": [
        "#####ATTNETION####"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def helper(master_df, tokenizer, num_examples=5):\n",
        "    samples = master_df[master_df['version'] == 'Bare'].head(num_examples)\n",
        "\n",
        "    for idx, row in samples.iterrows():\n",
        "        text = row['grammatical']\n",
        "        preverb = row['preverb']\n",
        "        verb = row['verb']\n",
        "\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "\n",
        "        print(f\"\\n--- Example {idx}: {preverb} + {verb} ---\")\n",
        "        print(f\"Full text: {text}\")\n",
        "        print(f\"Tokens: {tokens}\")\n",
        "\n",
        "        for i, t in enumerate(tokens):\n",
        "            if preverb.lower() in t.lower():\n",
        "                print(f\"  MATCH FOUND: '{t}' at index {i} matches Preverb '{preverb}'\")\n",
        "            if verb.lower() in t.lower():\n",
        "                print(f\"  MATCH FOUND: '{t}' at index {i} matches Verb '{verb}'\")\n",
        "\n",
        "# Run it\n",
        "helper(master_df, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0K4tBOZvZ_W",
        "outputId": "b5677c09-4a1f-4403-ab14-133ccc2ff0f3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 0: fel + menteni ---\n",
            "Full text: Nem mentette fel\n",
            "Tokens: ['▁Nem', '▁mentette', '▁fel']\n",
            "  MATCH FOUND: '▁fel' at index 2 matches Preverb 'fel'\n",
            "\n",
            "--- Example 1: meg + valósít ---\n",
            "Full text: Nem valósít meg\n",
            "Tokens: ['▁Nem', '▁valósít', '▁meg']\n",
            "  MATCH FOUND: '▁valósít' at index 1 matches Verb 'valósít'\n",
            "  MATCH FOUND: '▁meg' at index 2 matches Preverb 'meg'\n",
            "\n",
            "--- Example 2: ki + használja ---\n",
            "Full text: Ne használja ki\n",
            "Tokens: ['▁Ne', '▁használja', '▁ki']\n",
            "  MATCH FOUND: '▁használja' at index 1 matches Verb 'használja'\n",
            "  MATCH FOUND: '▁ki' at index 2 matches Preverb 'ki'\n",
            "\n",
            "--- Example 3: meg + felelt ---\n",
            "Full text: Nem felelt meg\n",
            "Tokens: ['▁Nem', '▁felelt', '▁meg']\n",
            "  MATCH FOUND: '▁felelt' at index 1 matches Verb 'felelt'\n",
            "  MATCH FOUND: '▁meg' at index 2 matches Preverb 'meg'\n",
            "\n",
            "--- Example 4: be + szervezte ---\n",
            "Full text: Nem szervezte be\n",
            "Tokens: ['▁Nem', '▁szervezte', '▁be']\n",
            "  MATCH FOUND: '▁szervezte' at index 1 matches Verb 'szervezte'\n",
            "  MATCH FOUND: '▁be' at index 2 matches Preverb 'be'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_targeted_attention_analysis(master_df, model_obj, tokenizer, target_words=[\"Nem\", \"Ne\", \"nem\", \"ne\"]):\n",
        "    model = model_obj.model if hasattr(model_obj, 'model') else model_obj\n",
        "    device = model.device\n",
        "    all_data = []\n",
        "\n",
        "    col_map = {\n",
        "        \"Bare\": (\"grammatical\", \"ungrammatical\"),\n",
        "        \"Context\": (\"grammatical_context\", \"ungrammatical_context\"),\n",
        "        \"Intervener\": (\"grammatical_context_adverb\", \"ungrammatical_context_adverb\"),\n",
        "        \"Intervener2\": (\"grammatical_context_adverb2\", \"ungrammatical_context_adverb2\")\n",
        "    }\n",
        "\n",
        "    def get_targeted_mass(text, preverb_str, verb_str):\n",
        "        if not isinstance(text, str) or not text.strip(): return None\n",
        "\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_attentions=True)\n",
        "        if outputs.attentions is None: return None\n",
        "\n",
        "        #source\n",
        "        source_idx = -1\n",
        "        for i, t in enumerate(tokens):\n",
        "            clean_t = t.replace('▁', '').lower()\n",
        "            if any(tgt.lower() == clean_t for tgt in target_words):\n",
        "                source_idx = i\n",
        "                break\n",
        "\n",
        "        if source_idx == -1: return None\n",
        "\n",
        "        #destination\n",
        "        critical_indices = []\n",
        "        for i, t in enumerate(tokens):\n",
        "            clean_t = t.replace('▁', '').lower()\n",
        "            if preverb_str.lower() in clean_t or verb_str.lower() in clean_t:\n",
        "                critical_indices.append(i)\n",
        "\n",
        "        if not critical_indices: return None\n",
        "\n",
        "        layer_masses = []\n",
        "        for layer_att in outputs.attentions:\n",
        "            att_tensor = layer_att[0]\n",
        "            att_matrix, _ = torch.max(att_tensor, dim=0) #maxpool\n",
        "\n",
        "            mass = sum(att_matrix[dest_i, source_idx].item() for dest_i in critical_indices)\n",
        "            layer_masses.append(mass)\n",
        "\n",
        "        return layer_masses\n",
        "\n",
        "    for _, row in tqdm(master_df.iterrows(), total=len(master_df), desc=\"Att Circuit\"):\n",
        "        version = row['version']\n",
        "        if version not in col_map: continue\n",
        "\n",
        "        for is_gram, col_name in [(True, col_map[version][0]), (False, col_map[version][1])]:\n",
        "            masses = get_targeted_mass(row[col_name], row['preverb'], row['verb'])\n",
        "            if masses:\n",
        "                for layer_i, m in enumerate(masses):\n",
        "                    all_data.append({\n",
        "                        \"Layer\": layer_i + 1,\n",
        "                        \"Attention_Mass\": m,\n",
        "                        \"Is_Grammatical\": is_gram,\n",
        "                        \"Version\": version\n",
        "                    })\n",
        "\n",
        "    return pd.DataFrame(all_data)\n",
        "\n",
        "print(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, output_attentions=True)\n",
        "df_attention_results = run_targeted_attention_analysis(master_df, model, tokenizer)\n"
      ],
      "metadata": {
        "id": "yvTZceUQ0y1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_att(df_results):\n",
        "    df_plot = df_results.copy()\n",
        "    df_plot[' '] = df_plot['Is_Grammatical'].map({\n",
        "        True: 'Grammatical',\n",
        "        False: 'Ungrammatical'\n",
        "    })\n",
        "\n",
        "    df_plot = df_plot[df_plot['Version'].isin(['Bare', 'Intervener2'])]\n",
        "\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    palette_colors = {'Grammatical': 'green', 'Ungrammatical': 'red'}\n",
        "\n",
        "    g = sns.relplot(\n",
        "        data=df_plot,\n",
        "        x=\"Layer\", y=\"Attention_Mass\",\n",
        "        hue=\" \",\n",
        "        col=\"Version\",\n",
        "        kind=\"line\",\n",
        "        palette=palette_colors,\n",
        "        marker=\"o\", linewidth=3,\n",
        "        errorbar=('ci', 95),\n",
        "        facet_kws={'sharey': True}\n",
        "    )\n",
        "\n",
        "    g.set_axis_labels(\"Model Layer\", \"Attention Mass (Max-Head)\")\n",
        "    g.set_titles(\"Experiment: {col_name}\", fontweight='bold')\n",
        "\n",
        "    plt.subplots_adjust(top=0.85)\n",
        "    g.figure.suptitle(\"Comparison of Attention Mass in Grammatical vs. Ungrammatical Minimal Pairs\", fontsize=14)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "visualize_att(df_attention_results)"
      ],
      "metadata": {
        "id": "-Ck1Gi_hp3Yi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPhL+XS5PbpQvLcVPK6N5m"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
